{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we demonstrate how to evaluate a trained event-SSM model on batches of unseen data on the three tasks:\n",
    " \n",
    "1) Spiking Speech Commands\n",
    "2) Spiking Heidelberg Digits\n",
    "3) DVS128 Gesture \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Setup\n",
    "\n",
    "Install and load the important modules and configuration.\n",
    "\n",
    "To install required packages, please do ``` pip3 install requirements.txt ``` <br>\n",
    "\n",
    "Directories for loading datasets, model checkpoints and saving results are defined in the configuration file `system/local.yaml`.\n",
    "Please set your directories accordingly.\n",
    "\n",
    "The trained model checkpoints are [available for download](https://datashare.tu-dresden.de/s/g2dQCi792B8DqnC).\n",
    "\n",
    "## Important Libraries\n",
    "* [Hydra](https://hydra.cc/docs/intro/) - to manage configurations.\n",
    "* [Flax](https://flax.readthedocs.io/en/latest/), Neural network package built on top of [Jax](https://jax.readthedocs.io/en/latest/) - for model development\n",
    "* [Tonic](https://tonic.readthedocs.io/en/latest/) - for datasets"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from typing import Optional, TypeVar, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import torch\n",
    "import tonic\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax.training import checkpoints\n",
    "\n",
    "from hydra import initialize, compose\n",
    "from omegaconf import OmegaConf as om\n",
    "\n",
    "from event_ssm.ssm import init_S5SSM\n",
    "from event_ssm.seq_model import BatchClassificationModel"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "os.environ['CUDA_VISIBLE_DEVICES'] = '-1' # Turn off GPU",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 - Spiking Heidelberg Digits\n",
    "\n",
    "Spike-based version of Heidelberg digits dataset, consist of approximately 10k high-quality recordings of spoken digits ranging from zero to nine in English and German language.  In total 12 speakers were included, six of which were female and six male. \n",
    "\n",
    "Two speakers were heldout exclusively for the test set. The remainder of the test set was filled with samples (5 % of the trials) from speakers also present in the training set.\n",
    "\n",
    "\n",
    "\n",
    "Ref : https://arxiv.org/pdf/1910.07407v3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 : Load configuration"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load configurations\n",
    "with initialize(version_base=None, config_path=\"configs\"):\n",
    "    cfg = compose(config_name=\"base.yaml\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# See the model config:\n",
    "print(om.to_yaml(cfg.model))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 : Visualise data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "data = tonic.datasets.SHD(cfg.data_dir, train=False)\n",
    "audio_events, label = data[0]\n",
    "tonic.utils.plot_event_grid(audio_events)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Load single data sample for inference"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "DEFAULT_CACHE_DIR_ROOT = Path('./cache_dir/')\n",
    "DataLoader = TypeVar('DataLoader')\n",
    "InputType = [str, Optional[int], Optional[int]]\n",
    "class Data:\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_classes: int,\n",
    "            num_embeddings: int\n",
    "            ):\n",
    "        self.n_classes = n_classes\n",
    "        self.num_embeddings = num_embeddings"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def event_stream_collate_fn(batch, resolution, pad_unit, no_time_information=False):\n",
    "    # x are inputs, y are targets, z are aux data\n",
    "    x, y, *z = zip(*batch)\n",
    "    assert len(z) == 0\n",
    "    batch_size_one = len(x) == 1\n",
    "\n",
    "    # set labels to numpy array\n",
    "    y = np.stack(y)\n",
    "\n",
    "    # integration time steps are the difference between two consequtive time stamps\n",
    "    if no_time_information:\n",
    "        timesteps = [np.ones_like(e['t'][:-1]) for e in x]\n",
    "    else:\n",
    "        timesteps = [np.diff(e['t']) for e in x]\n",
    "\n",
    "    # NOTE: since timesteps are deltas, their length is L - 1, and we have to remove the last token in the following\n",
    "\n",
    "    # process tokens for single input dim (e.g. audio)\n",
    "    if len(resolution) == 1:\n",
    "        tokens = [e['x'][:-1].astype(np.int32) for e in x]\n",
    "    elif len(resolution) == 2:\n",
    "        tokens = [(e['x'][:-1] * e['y'][:-1] + np.prod(resolution) * e['p'][:-1].astype(np.int32)).astype(np.int32) for e in x]\n",
    "    else:\n",
    "        raise ValueError('resolution must contain 1 or 2 elements')\n",
    "\n",
    "    # get padding lengths\n",
    "    lengths = np.array([len(e) for e in timesteps], dtype=np.int32)\n",
    "    pad_length = (lengths.max() // pad_unit) * pad_unit + pad_unit\n",
    "\n",
    "    # pad tokens with -1, which results in a zero vector with embedding look-ups\n",
    "    tokens = np.stack(\n",
    "        [np.pad(e, (0, pad_length - len(e)), mode='constant', constant_values=-1) for e in tokens])\n",
    "    timesteps = np.stack(\n",
    "        [np.pad(e, (0, pad_length - len(e)), mode='constant', constant_values=0) for e in timesteps])\n",
    "\n",
    "    # timesteps are in micro seconds... transform to milliseconds\n",
    "    timesteps = timesteps / 1000\n",
    "\n",
    "    if batch_size_one:\n",
    "        lengths = lengths[None, ...]\n",
    "\n",
    "    return tokens, y, timesteps, lengths"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def event_stream_dataloader(test_data,eval_batch_size,eval_collate_fn, rng, num_workers=0):\n",
    "    def dataloader(dset, bsz, collate_fn, shuffle, drop_last):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            dset,\n",
    "            batch_size=bsz,\n",
    "            drop_last=drop_last,\n",
    "            collate_fn=collate_fn,\n",
    "            shuffle=shuffle,\n",
    "            generator=rng,\n",
    "            num_workers=num_workers\n",
    "        )\n",
    "    test_loader = dataloader(test_data, eval_batch_size, eval_collate_fn, shuffle=True, drop_last=False)\n",
    "    return test_loader"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def create_events_shd_classification_dataset(\n",
    "        cache_dir: Union[str, Path] = DEFAULT_CACHE_DIR_ROOT,\n",
    "        per_device_eval_batch_size: int = 64,\n",
    "        world_size: int = 1,\n",
    "        num_workers: int = 0,\n",
    "        seed: int = 42,\n",
    "        pad_unit: int = 8192,\n",
    "        no_time_information: bool = False,\n",
    "        **kwargs\n",
    ") -> Tuple[DataLoader, Data]:\n",
    "    \"\"\"\n",
    "    creates a view of the spiking heidelberg digits dataset\n",
    "\n",
    "    :param cache_dir:\t\t                        (str):\t\twhere to store the dataset\n",
    "    :param per_device_eval_batch_size:\t\t\t\t(int):\t\tEvaluation Batch size.\n",
    "    :param seed:\t\t\t                        (int):\t\tSeed for shuffling data.\n",
    "    \"\"\"\n",
    "    print(\"[*] Generating Spiking Heidelberg Digits Classification Dataset\")\n",
    "\n",
    "    if seed is not None:\n",
    "        rng = torch.Generator()\n",
    "        rng.manual_seed(seed)\n",
    "    else:\n",
    "        rng = None\n",
    "    \n",
    "    #target_transforms = OneHotLabels(num_classes=20)\n",
    "    test_data = tonic.datasets.SHD(save_to=cache_dir, train=False)\n",
    "    collate_fn = partial(event_stream_collate_fn, resolution=(700,), pad_unit=pad_unit, no_time_information=no_time_information)\n",
    "    test_loader = event_stream_dataloader(\n",
    "        test_data,\n",
    "        eval_collate_fn=collate_fn,\n",
    "        eval_batch_size=per_device_eval_batch_size * world_size,\n",
    "        rng=rng, \n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    data = Data(\n",
    "        n_classes=20, num_embeddings=700)\n",
    "    return test_loader, data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"[*] Loading dataset...\")\n",
    "num_devices = jax.local_device_count()\n",
    "test_loader, data = create_events_shd_classification_dataset(\n",
    "        cache_dir=cfg.data_dir,\n",
    "        seed=cfg.seed,\n",
    "        world_size=num_devices,\n",
    "        per_device_eval_batch_size = 1,\n",
    "        pad_unit=cfg.training.pad_unit        \n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load a sample\n",
    "batch = next(iter(test_loader))\n",
    "inputs, targets, timesteps, lengths = batch"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 : Load model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Set the random seed manually for reproducibility.\n",
    "init_key = jax.random.PRNGKey(cfg.seed)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Model initialisation in flax\n",
    "ssm_init_fn = init_S5SSM(**cfg.model.ssm_init)\n",
    "model = BatchClassificationModel(\n",
    "        ssm=ssm_init_fn,\n",
    "        num_classes=data.n_classes,\n",
    "        num_embeddings=data.num_embeddings,\n",
    "        **cfg.model.ssm,\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualise model\n",
    "print(model.tabulate({\"params\": init_key},\n",
    "        inputs, timesteps, lengths, False))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "checkpoint_dir = os.path.abspath(os.path.join(cfg.checkpoint_dir, 'SHD'))\n",
    "training_state = checkpoints.restore_checkpoint(checkpoint_dir, target=None)\n",
    "params = training_state['params']\n",
    "model_state = training_state['model_state']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 - Model prediction"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "logits = model.apply({'params': params, **model_state}, inputs, timesteps, lengths, False)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(f\"Predicted label:{jnp.argmax(logits,axis=-1)}\")\n",
    "print(f\"Actual label:{targets}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 - Evaluate model on a batch"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"[*] Loading dataset...\")\n",
    "num_devices = jax.local_device_count()\n",
    "test_loader, data = create_events_shd_classification_dataset(\n",
    "        cache_dir=cfg.data_dir,\n",
    "        seed=cfg.seed,\n",
    "        world_size=num_devices,\n",
    "        per_device_eval_batch_size = cfg.training.per_device_eval_batch_size,\n",
    "        pad_unit=cfg.training.pad_unit,\n",
    "        #no_time_information = cfg.training.no_time_information\n",
    "        \n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load a batch\n",
    "batch = next(iter(test_loader))\n",
    "inputs, targets, timesteps, lengths = batch\n",
    "logits = model.apply({'params': params, **model_state},inputs, timesteps, lengths,False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot the confusion matrix\n",
    "cm = confusion_matrix(jnp.argmax(logits,axis=1), targets)\n",
    "sns.heatmap(cm, annot=True,fmt='d', cmap='YlGnBu')\n",
    "plt.ylabel('Prediction',fontsize=12)\n",
    "plt.xlabel('Actual',fontsize=12)\n",
    "plt.title('Confusion Matrix',fontsize=16)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(f\"Accuracy of the model: {(jnp.argmax(logits,axis=1)==targets).mean()}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 - Spiking Speech Commands \n",
    "\n",
    "The Spiking Speech Commands is based on the Speech Commands release by Google which consists of utterances recorded from a larger number of speakers under less controlled conditions. It contains 35 word categories from a larger number of speakers.\n",
    "\n",
    "Ref : https://arxiv.org/pdf/1910.07407v3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 : Load configuration"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load configurations\n",
    "with initialize(version_base=None, config_path=\"configs\"):\n",
    "    cfg = compose(config_name=\"base.yaml\",overrides=[\"task=spiking-speech-commands\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# See the model config:\n",
    "print(om.to_yaml(cfg.model))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 : Visualise data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "data = tonic.datasets.SSC(cfg.data_dir, split='test')\n",
    "audio_events, label = data[0]\n",
    "tonic.utils.plot_event_grid(audio_events)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Load single data sample for inference"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def create_events_ssc_classification_dataset(\n",
    "        cache_dir: Union[str, Path] = DEFAULT_CACHE_DIR_ROOT,\n",
    "        per_device_eval_batch_size: int = 64,\n",
    "        world_size: int = 1,\n",
    "        num_workers: int = 0,\n",
    "        seed: int = 42,\n",
    "        pad_unit: int = 8192,\n",
    "        no_time_information: bool = False,\n",
    "        **kwargs\n",
    ") -> Tuple[DataLoader, DataLoader, DataLoader, Data]:\n",
    "    \"\"\"\n",
    "    creates a view of the spiking speech commands dataset\n",
    "\n",
    "    :param cache_dir:\t\t(str):\t\twhere to store the dataset\n",
    "    :param bsz:\t\t\t\t(int):\t\tBatch size.\n",
    "    :param seed:\t\t\t(int)\t\tSeed for shuffling data.\n",
    "    \"\"\"\n",
    "    print(\"[*] Generating Spiking Speech Commands Classification Dataset\")\n",
    "\n",
    "    if seed is not None:\n",
    "        rng = torch.Generator()\n",
    "        rng.manual_seed(seed)\n",
    "    else:\n",
    "        rng = None\n",
    "\n",
    "    test_data = tonic.datasets.SSC(save_to=cache_dir, split='test')\n",
    "    collate_fn = partial(event_stream_collate_fn, resolution=(700,), pad_unit=pad_unit, no_time_information=no_time_information)\n",
    "    test_loader = event_stream_dataloader(\n",
    "        test_data,\n",
    "        eval_collate_fn=collate_fn,\n",
    "        eval_batch_size=per_device_eval_batch_size * world_size,\n",
    "        rng=rng, \n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    data = Data(\n",
    "        n_classes=35, num_embeddings=700\n",
    "    )\n",
    "    return test_loader, data\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"[*] Loading dataset...\")\n",
    "num_devices = jax.local_device_count()\n",
    "test_loader, data = create_events_ssc_classification_dataset(\n",
    "        cache_dir=cfg.data_dir,\n",
    "        seed=cfg.seed,\n",
    "        world_size=num_devices,\n",
    "        per_device_eval_batch_size = 1,\n",
    "        pad_unit=cfg.training.pad_unit        \n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load a sample\n",
    "batch = next(iter(test_loader))\n",
    "inputs, targets, timesteps, lengths = batch"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 : Load model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Set the random seed manually for reproducibility.\n",
    "init_key = jax.random.PRNGKey(cfg.seed)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ssm_init_fn = init_S5SSM(**cfg.model.ssm_init)\n",
    "model = BatchClassificationModel(\n",
    "        ssm=ssm_init_fn,\n",
    "        num_classes=data.n_classes,\n",
    "        num_embeddings=data.num_embeddings,\n",
    "        **cfg.model.ssm,\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(model.tabulate({\"params\": init_key},\n",
    "        inputs, timesteps, lengths, False))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# load model parameters from checkpoint\n",
    "checkpoint_dir = os.path.abspath(os.path.join(cfg.checkpoint_dir, 'SSC'))\n",
    "training_state = checkpoints.restore_checkpoint(checkpoint_dir, target=None)\n",
    "params = training_state['params']\n",
    "model_state = training_state['model_state']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 - Model prediction"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "logits = model.apply({'params': params, **model_state},inputs, timesteps, lengths,False)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(f\"Predicted label:{jnp.argmax(logits,axis=-1)}\")\n",
    "print(f\"Actual label:{targets}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 - Evaluate model on single batch"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"[*] Loading dataset...\")\n",
    "num_devices = jax.local_device_count()\n",
    "test_loader, data = create_events_ssc_classification_dataset(\n",
    "        cache_dir=cfg.data_dir,\n",
    "        seed=cfg.seed,\n",
    "        world_size=num_devices,\n",
    "        per_device_eval_batch_size = cfg.training.per_device_eval_batch_size,\n",
    "        pad_unit=cfg.training.pad_unit,\n",
    "        #no_time_information = cfg.training.no_time_information\n",
    "        \n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load a batch\n",
    "batch = next(iter(test_loader))\n",
    "inputs, targets, timesteps, lengths = batch"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "logits = model.apply({'params': params, **model_state},inputs, timesteps, lengths,False)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot the confusion matrix\n",
    "cm = confusion_matrix(jax.numpy.argmax(logits,axis=-1), targets)\n",
    "sns.heatmap(cm, annot=True,fmt='d', cmap='YlGnBu')\n",
    "plt.ylabel('Prediction',fontsize=12)\n",
    "plt.xlabel('Actual',fontsize=12)\n",
    "plt.title('Confusion Matrix',fontsize=16)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "print(f\"Accuracy of the model: {(jnp.argmax(logits,axis=-1)==targets).mean()}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 - DVS Gesture "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Description\n",
    "\n",
    "It is the first gesture recognition system implemented end-to-end on event-based hardware. The dataset comprises of 11 hand gesture categories from 29 subjects under 3 illumination conditions.\n",
    "\n",
    "Ref : https://ieeexplore.ieee.org/document/8100264\n",
    "\n",
    "### Excercise\n",
    "\n",
    "Similar to SHD and SSC, implement inference steps for DVS Gesture data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 : Load configuration"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load configurations\n",
    "with initialize(version_base=None, config_path=\"configs\"):\n",
    "    cfg = compose(config_name=\"base.yaml\",overrides=[\"task=dvs-gesture\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# model config:\n",
    "print(om.to_yaml(cfg.model))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 : Visualise Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from IPython.display import HTML\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#warnings.filterwarnings( \"ignore\", module = \"matplotlib\\..*\" )\n",
    "\n",
    "data = tonic.datasets.DVSGesture(cfg.data_dir, train=False)\n",
    "events, label = data[0]\n",
    "\n",
    "transform = tonic.transforms.Compose(\n",
    "    [\n",
    "        tonic.transforms.TimeJitter(std=100, clip_negative=False),\n",
    "        tonic.transforms.ToFrame(\n",
    "        sensor_size=data.sensor_size,\n",
    "        time_window=10000,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "frames = transform(events)\n",
    "HTML(tonic.utils.plot_animation((frames* 255).astype(np.uint8)).to_html5_video())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Load single inference sample"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from event_ssm.transform import Identity"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def create_events_dvs_gesture_classification_dataset(\n",
    "        cache_dir: Union[str, Path] = DEFAULT_CACHE_DIR_ROOT,\n",
    "        per_device_eval_batch_size: int = 64,\n",
    "        world_size: int = 1,\n",
    "        num_workers: int = 0,\n",
    "        seed: int = 42,\n",
    "        pad_unit: int = 2 ** 19,\n",
    "        downsampling: int=1,\n",
    "        **kwargs\n",
    ") -> Tuple[DataLoader, Data]:\n",
    "    \"\"\"\n",
    "    creates a view of the DVS Gesture dataset\n",
    "\n",
    "    :param cache_dir:\t\t(str):\t\twhere to store the dataset\n",
    "    :param bsz:\t\t\t\t(int):\t\tBatch size.\n",
    "    :param seed:\t\t\t(int)\t\tSeed for shuffling data.\n",
    "    \"\"\"\n",
    "    print(\"[*] Generating DVS Gesture Classification Dataset\")\n",
    "\n",
    "    if seed is not None:\n",
    "        rng = torch.Generator()\n",
    "        rng.manual_seed(seed)\n",
    "    else:\n",
    "        rng = None\n",
    "\n",
    "    orig_sensor_size = (128, 128, 2)\n",
    "    new_sensor_size = (128 // downsampling, 128 // downsampling, 2)\n",
    "    test_transforms = tonic.transforms.Compose([\n",
    "        tonic.transforms.Downsample(sensor_size=orig_sensor_size, target_size=new_sensor_size[:2]) if downsampling > 1 else Identity(),\n",
    "    ])\n",
    "\n",
    "    TestData = partial(tonic.datasets.DVSGesture, save_to=cache_dir, train=False)\n",
    "    test_data = TestData(transform=test_transforms)\n",
    "\n",
    "    # define collate function\n",
    "    eval_collate_fn = partial(\n",
    "            event_stream_collate_fn,\n",
    "            resolution=new_sensor_size[:2],\n",
    "            pad_unit=pad_unit,\n",
    "        )\n",
    "    test_loader = event_stream_dataloader(\n",
    "        test_data,\n",
    "        eval_collate_fn=eval_collate_fn,\n",
    "        eval_batch_size=per_device_eval_batch_size * world_size,\n",
    "        rng=rng, \n",
    "        num_workers=num_workers\n",
    "        )\n",
    "\n",
    "    data = Data(\n",
    "        n_classes=11, num_embeddings=np.prod(new_sensor_size)\n",
    "    )\n",
    "    return test_loader, data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "num_devices = jax.local_device_count()\n",
    "    # Create dataset...\n",
    "test_loader, data = create_events_dvs_gesture_classification_dataset(\n",
    "        cache_dir=cfg.data_dir,\n",
    "        seed=cfg.seed,\n",
    "        world_size=num_devices,\n",
    "        per_device_eval_batch_size = 1,\n",
    "        pad_unit=cfg.training.pad_unit,        \n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load a sample\n",
    "batch = next(iter(test_loader))\n",
    "inputs, targets, timesteps, lengths = batch"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 : Load model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Set the random key for the task\n",
    "init_key = jax.random.PRNGKey(cfg.seed)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"[*] Creating model...\")\n",
    "ssm_init_fn = init_S5SSM(**cfg.model.ssm_init)\n",
    "model = BatchClassificationModel(\n",
    "        ssm=ssm_init_fn,\n",
    "        num_classes=data.n_classes,\n",
    "        num_embeddings=data.num_embeddings,\n",
    "        **cfg.model.ssm,\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# visualise model\n",
    "print(model.tabulate({\"params\": init_key},\n",
    "        inputs, timesteps, lengths, False))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# load model parameters from checkpoint\n",
    "checkpoint_dir = os.path.abspath(os.path.join(cfg.checkpoint_dir, 'DVS'))\n",
    "training_state = checkpoints.restore_checkpoint(checkpoint_dir, target=None)\n",
    "params = training_state['params']\n",
    "model_state = training_state['model_state']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 - Model prediction"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "logits = model.apply({'params': params, **model_state}, inputs, timesteps, lengths, False)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(f\"Predicted label:{jnp.argmax(logits,axis=-1)}\")\n",
    "print(f\"Actual label:{targets}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 - Evaluate model on single batch"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "num_devices = jax.local_device_count()\n",
    "    # Create dataset...\n",
    "test_loader, data = create_events_dvs_gesture_classification_dataset(\n",
    "        cache_dir=cfg.data_dir,\n",
    "        seed=cfg.seed,\n",
    "        world_size=num_devices,\n",
    "        per_device_eval_batch_size = cfg.training.per_device_eval_batch_size,\n",
    "        pad_unit=cfg.training.pad_unit,\n",
    "        #no_time_information = cfg.training.no_time_information\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load a batch\n",
    "batch = next(iter(test_loader))\n",
    "inputs, targets, timesteps, lengths = batch"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": "logits = model.apply({'params': params, **model_state}, inputs, timesteps, lengths, False)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot the confusion matrix\n",
    "cm = confusion_matrix(jax.numpy.argmax(logits, axis=-1), targets)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='YlGnBu')\n",
    "plt.ylabel('Prediction', fontsize=12)\n",
    "plt.xlabel('Actual', fontsize=12)\n",
    "plt.title('Confusion Matrix', fontsize=16)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "print(f\"Accuracy of the model: {(jnp.argmax(logits, axis=-1) == targets).mean()}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blocksparse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
